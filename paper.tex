\documentclass[10pt, letterpaper]{article}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}

%%%%%%%%%%%%%%%%%%%%%%% Title info %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{Kiran Vodrahalli, Evelyn Ding, Albert Lee}
\title{Word Sense Disambiguation: Pipelining with Supervised LSA}
\date{May 8, 2014}

\begin{document}

	\maketitle
	
	\section{Introduction}
	\subsection{Problem Statement}
	Word sense disambiguation is the process of identifying the meaning of a word in its context when an individual word may take on many meanings.
	It remains one of the oldest open problems in computational linguistics and is widely considered to be AI-Complete, 
	or as hard as the problem of making computers as intelligent as people.$^1$ [CITATION] Being able to accurately disambiguate word senses would greatly
	assist Machine Translation, semantic analysis, and would have significant impact in computational linguistics. However, fine-grained senses
	in which a word has multiple closely related meanings remains one of the largest challenges in word sense disambiguation. Consider the following
	example of disambiguating the sense of the word {\emph {art}} from the Senseval-2 competition held in 2001:
	\begin{enumerate}
	\item "Paintings, drawings and sculpture from every period of {\emph {art}} during the last 350 years will be on display ranging from a Tudor portrait to contemporary British art."
	\item "Through casual meetings at cafe's, the artists drew together to form a movement in protest against the waste of war, against nationalism and against everything pompous, conventional or boring in the {\emph {art}} of the Western world."
	\end{enumerate}
	In the first example, {\emph {art}} refers to {\emph {the creation of beautiful or significant things}} whereas in the second example, it refers to {\emph {the products of human creativity; works of art collectively}}. Such subtle distinctions are part of what makes word sense disambiguation a challenging problem that remains open to this day.
	\section{Literature Survey}
	knowledge based

	unsupervised corpus-based

	supervised corpus-based
	\subsection{Knowledge-based}
	Lesk's algorithm:
	two words W1 and W2 with multiple word senses
	for each sense i in W1
	for each sense j in W2
	calculate overlap of i and j in dictionary definitions
	maximize overlap i and j to determine word senses
	Semantic similarity:
	calculate path between words in wordnet for each word sense with other words in the context
	minimize total semantic path to select word sense
	context can vary from one sentence to length of document
	
	\subsection{Unsupervised Corpus-based}
	 Token-based discrimination: cluster contexts in which a given target word appears
	 Cluster 1: The line was occupied. 
	 The operator came into the line abruptly.

	 Cluster 2: The line was really long and it took forever to be served. 
 	 I stood in the queue for about 10 minutes

 	 Use parallel corpus across language to infer sense distinctions across the language
	 Je vais prendre ma propre dÃ©cision.
	 vs.
	 Je vais prendre ma propre voiture.
	\subsection{Supervised Corpus-based}
	 Use annotated corpus to build model
 	 ex. to know: 1) be aware of piece of information
                              I want to know who is winning.
                              I know that the president lied.
                             2) know person
                             She doesn't know the composer.
                             Do you know my sister?
	 Use context of how word usually appears (bigram/trigram models)
 	 if (feature) then word sense
 	\subsection{LSA -- Past Efforts}
 	LSA was one of the first approaches to make use of the co-occurrence model for word meaning. The idea, as implemented in the famous 1998 paper by Landauer [CITATION], is that given a set of corpuses, we build a term-document matrix where words are on the rows and documents are on the columns, and the frequencies are in the corresponding cells.
	Then we apply SVD to this matrix. 
	This approach is essentially unsupervised because it relies on clustering (word co-occurrence matrix) to derive similarities between documents and words. 

	Originally, the LSA approach was primarily intended to remove uninformative singular values, and then reconstruct the term-document matrix (Landauer et al 1998) [CITATION].  The idea there was to compare documents to see how similar they were to each other with noise removed. Part of their motivation was to see if co-occurrence representations was in fact how humans represented meaning themselves.

	(Katz et al) first applied LSA to WSD in an unsupervised approach by folding new documents with ambiguous word into the semantic space created by the SVD of the term-document matrix, and then used cosine-based clustering [CITATION]. We do a supervised implementation of this approach. 

	It is worth noting that other similar approaches exist in recent years that are probably more powerful (but more difficult to implement). We detail them in the following sections. 

	\subsubsection{Probabilistic LSA}

	-- Formulated as early as 1999, this approach is based on mixture decomposition from latent class model. 
	(this is a latent variable model for co-occurrence data)
	Model fitting is done with Expectation Maximization (EM). [CITATION]

	\subsubsection{Non-Negative Matrix Factorization}
	-- An alternative to using SVD is NNMF, which is a matrix factoring technique introduced by Lee and Seung  (2000) [CITATION].  Van de Cruys (2011) makes use of this technique [CITATION]:
	-- minimize Kullback-Leibler divergence instead of Euclidean distance

	-- minimizing Kullback-Leibler divergence is better, since minimizing Euclidean distance makes the normal distribution assumption for language -- language is not Gaussian.
	-- useful for extraction of semantic dimension 
	-- turns out to be useful to have no negative relations encoded (all matrix values are positive)

	Van de Cruys (2011): uses this approach on SEMEVAL-2010 dataset. The rest of the approach is analogous to SVD-LSA, computing the centroid  of candidate vectors and comparing via Kullback-Leibler divergence to the target vector to determine word sense. 

	\section{Data}
	Senseval-2 (2001) dataset [CITATION]
	73 words that need to be disambiguated
	         ex. fine, art, pull, turn, carry
	Up to 43 senses per word
	8611 labeled training instances
	4328 test instances	

	brown corpus

	\section{Algorithms}
	 Our approach was essentially a pipeline of different sorts of models. 
	 The first stage consisted of part-of-speech tagging and bigram models, which
	 was used for the subset of data that had high predictability. 
	 Originally, we implemented a step that included knowledge-base approaches like
	 semantic similarity, but later discarded it from the pipeline because its accuracy
	 was not very good. 

	 For the rest of the data, we implemented a Supervised LSA approach, which was to
	 an extent a novel approach. 

	 We detail the methods in the following sections. 
 	\subsection{Part-of-Speech}
 	\subsection{Semantic Knowledgebase}
        Lesk
        Semantic similarity
	\subsection{Supervised LSA}
	Our approach uses simple LSA with a supervised learning component. 

	The steps are as follows: 

	\begin{enumerate}

	\item Build term-document matrix 
		\begin{enumerate}
		\item terms are words in Brown Corpus and in training data
		\item documents are categories of Brown, and concatenated paragraphs of training data with same word sense for ambiguous word
		\end{enumerate}
	\item Apply SVD to get $U$, $D$, $V^T$.

	Then we have the supervised aspect of the LSA.  
	\item For each (paragraph, ambiguous word, word sense) tuple in the training data:
		\begin{enumerate}
		\item create vector in the term-document space
		\item fold this vector into the reduced semantic space of V
		\item use cosine similarity to come up with the eigentopic most associated with the paragraph.
		\end{enumerate} 
	Then, we have built an association between each word sense and an eigentopic vector in V. 
	\item For each (paragraph, ambiguous word) in testing data:
		\begin{enumerate}
		\item come up with the eigentopic vector associated
		\item For that eigentopic vector, there is a probability for each word sense for a given word -- we pick the highest probability word sense for the word we are disambiguating
		\end{enumerate}

	\end{enumerate}

	In other words, for a given eigentopic, we will always pick the same word sense for a given word. This approach has its flaws.

	We also tried a slightly different approach to capture more of the information: For each word, and for each sense of the word, we created a vector by applying cosine similarity to each eigentopic column vector for our training vector (created from paragraph). We averaged over all the training data for each word sense. Then for testing, we created this vector from the test paragraph and compared via cosine similarity again to determine which word sense was most likely. 
	This approach had issues and we got very bad results. 
	It might be better in the future to try a modified approach to this (i.e. do something other than average the vectors).

	\section{Results}
	From Senseval-2:

	Best overall (supervised approach): 64$\%$	

	Best unsupervised approach: 40$\%$

	\subsection{POS Bigram Model}

	POS Bigram Model (w/frequency count $>$ 3): 
	\begin{table}[h]
	\begin{tabular}{|l|l|l|l|}
	\hline
	Threshold            & 80\%   & 40\%   & 0\%    \\ \hline
	Correctly identified & 199    & 518    & 574    \\ \hline
	Total identified     & 253    & 893    & 1136   \\ \hline
	\% Accuracy          & 78.7\% & 58.0\% & 50.5\% \\ \hline
	\% Coverage			 & 5.8\%  & 20.6\% & 26.2\% \\ \hline
	\end{tabular}
	\end{table}\\
	POS Bigram Model (w/frequency count $>$ 0)
	\begin{table}[h]
	\begin{tabular}{|l|l|l|l|}
	\hline
	Threshold            & 80\%   & 40\%   & 0\%    \\ \hline
	Correctly identified & 466    & 808    & 850    \\ \hline
	Total identified     & 898    & 1641   & 1826   \\ \hline
	\% Accuracy          & 51.9\% & 49.2\% & 46.5\% \\ \hline
	\% Coverage          & 20.7\% & 38.7\% & 42.2\% \\ \hline
	\end{tabular}
	\end{table}

	Correctly disambiguates:
	"Marketing too, in its strictest sense, is outside our remit. " (the way in which a word or expression or situation can be interpreted)	

	"Before he lost his money he had the good sense to commission John Soane to design the Campden Hill Square house." (a general conscious awareness)

	\subsection{Supervised LSA}

	a) With only the Brown Corpus categories as column vectors ($15$ column vectors -> reduced to $9$) in the term-document matrix: ~ $42\%$
	b) With the Brown Corpus categories + the word-sense categories (in total, 861 column vectors -> reduced to $140$ column vectors after SVD): ~ $38\%$
	-- highly dependent on the training data -- different results for different words, some words had lower percentages, some had higher percentages (as low as $7\%$, as high as $61\%$)
	-- easier to do ambiguous words with fewer senses

	\section{Conclusions}
	One issue with the LSA approach may be that the corpuses we use (namely, for a given word and a given sense, all the training paragraphs concatenated together) may actually cover a very broad array of topics, and thus a lot of noise might be added. While this should theoretically be removed by SVD, it might just be that the 'paragraph topics' were too disjoint and the correlation for different very fine-tuned usage is not strong enough. 
	Examples of failures: 

	for the word "fine": 	

	our program: "elegant"  actual: "thin (in thickness)"
	our program: "superlative" actual: "elegant" 	

	and so on... it makes sense that both senses could apply to some of the testing data

	Labeled training data sparsity is a large issue

	Some statistical methods fall short of disambiguating subtle nuances between word senses	

	The LSA approach is primarily topic categorization -- we need to use better documents that are more tailored to WSD, perhaps. 	

	It is hard to use a topic-categorization approach to do fine-tuned WSD. 

	\section{Future Work}
	Could try different document columns for Supervised LSA.
	Could try applying supervised approach to PLSA or NNMF-LSA.	

	Recall Socher et al. (2012) -- Recursive Neural Nets on sentiment trees
	Integrate with Socher's work on sentiment analysis
	Found that Socher doesn't disambiguate across certain words such as "mean", "like"
	Could improve accuracy of sentiment analysis	

	Can be applied to translation if WSD is made better

	Maybe could apply SVM to the (word sense: cosine-vector) approach (i.e. train an SVM 
	for every ambiguous word in the training data with the cosine vectors as input)

	\section{Citations} 
	.

	1. Agirre, Eneko, and Philip Glenny. Edmonds. Word Sense Disambiguation: Algorithms and Applications. Dordrecht: Springer, 2006. Print.

	2. Phil Katz and Paul Goldsmith-Pinkham. 2006. Word Sense Disambiguation Using Latent Semantic Analysis Retrieved from http://www.sccs.swarthmore.edu/users/07 /pkatz1/cs65f06-final.pdf.

	3. T.K. Landauer, Foltz P.W., and D. Laham. 1998. Introduction to latent semantic analysis. $\textit{Discourse Processes}$, $25:259-284$. 

	4. Thomas Hofmann. 1999. Probabilistic Latent Semantic Analysis. Retrieved from http://cs.brown.edu/~th/papers/Hofmann-UAI99.pdf. 

	5. Tim Van de Cruys and Marianna Apidianaki. 2011. Latent Semantic Word Sense Induction and Disambiguation. Retrieved from http://aclweb.org/anthology//P/P11/P11-1148.pdf. 

	6. Kirk Baker. 2013. Singular Value Decomposition Tutorial. Retrieved from http://www.ling.ohio-state. edu/~kbaker/pubs/Singular$\_$Value$\_$Decomposition$\_$Tutorial.pdf. 

	7. Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. OâReilly Media Inc.

	8. Princeton University "About WordNet." WordNet. Princeton University. 2010. <http://wordnet.princeton.edu>

	9. ACL-SIGLEX. 2001. Evaluation Exercises for the Semantic Analysis of Text. Retrieved from http://www.senseval.org/. 

	\section{Code}

	The code we used in this paper is available at https://github.com/alee101/wsd. 




 


\end{document}
